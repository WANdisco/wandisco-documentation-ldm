(window.webpackJsonp=window.webpackJsonp||[]).push([[127],{199:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return o})),a.d(t,"metadata",(function(){return i})),a.d(t,"toc",(function(){return c})),a.d(t,"default",(function(){return s}));var r=a(3),n=(a(0),a(237));const o={id:"ldm-trial-for-aws",title:"LiveData Migrator Trial for AWS"},i={unversionedId:"ldm-trial-for-aws",id:"version-1.8.3/ldm-trial-for-aws",isDocsHomePage:!1,title:"LiveData Migrator Trial for AWS",description:"If you want to get familiar with LiveData Migrator before committing to a production implementation, you can use a HDFS Sandbox cluster as your source filesystem for LiveData Migrator.",source:"@site/versioned_docs/version-1.8.3/ldm-trial-for-aws.md",slug:"/ldm-trial-for-aws",permalink:"/wandisco-documentation-ldm/docs/1.8.3/ldm-trial-for-aws",version:"1.8.3"},c=[{value:"Prerequisites",id:"prerequisites",children:[]},{value:"Deploy the Sandbox",id:"deploy-the-sandbox",children:[]},{value:"Install and configure LiveData Migrator",id:"install-and-configure-livedata-migrator",children:[]},{value:"Start a migration",id:"start-a-migration",children:[]},{value:"Confirm the migration is successful",id:"confirm-the-migration-is-successful",children:[]}],l={toc:c};function s({components:e,...t}){return Object(n.b)("wrapper",Object(r.a)({},l,t,{components:e,mdxType:"MDXLayout"}),Object(n.b)("p",null,"If you want to get familiar with LiveData Migrator before committing to a production implementation, you can use a HDFS Sandbox cluster as your source filesystem for LiveData Migrator."),Object(n.b)("p",null,"The HDFS Sandbox for LiveData Migrator is a non-kerberized ",Object(n.b)("a",{parentName:"p",href:"https://www.cloudera.com/downloads/hortonworks-sandbox.html"},"HDP 2.6.5 Sandbox")," Docker environment."),Object(n.b)("p",null,"The following steps explain how to deploy the HDFS Sandbox and perform a migration of data to your S3 bucket using LiveData Migrator."),Object(n.b)("h2",{id:"prerequisites"},"Prerequisites"),Object(n.b)("p",null,"Follow our guide to create an AWS EC2 instance that has the correct dependencies installed to host the HDP Sandbox:"),Object(n.b)("p",null,Object(n.b)("a",{parentName:"p",href:"https://wandisco.github.io/wandisco-documentation/docs/quickstarts/preparation/aws_vm_creation/"},"AWS VM Creation")),Object(n.b)("p",null,"Alternatively, create the host through the AWS portal or host the Sandbox on your own server. The requirements for the host are as follows:"),Object(n.b)("ul",null,Object(n.b)("li",{parentName:"ul"},"Linux Server",Object(n.b)("ul",{parentName:"li"},Object(n.b)("li",{parentName:"ul"},"Minimum size recommendation = ",Object(n.b)("strong",{parentName:"li"},"8 CPUs, 32 GiB memory"),"."),Object(n.b)("li",{parentName:"ul"},"A minimum of 24GB available storage for the ",Object(n.b)("inlineCode",{parentName:"li"},"/var/lib/docker")," directory."),Object(n.b)("li",{parentName:"ul"},"Network connectivity to your S3 bucket."),Object(n.b)("li",{parentName:"ul"},"Port 8080 must be accessible to access the Ambari UI."),Object(n.b)("li",{parentName:"ul"},"Port 8081 must be accessible to access the LiveData UI.")))),Object(n.b)("p",null,"The following services must be installed:"),Object(n.b)("ul",null,Object(n.b)("li",{parentName:"ul"},Object(n.b)("a",{parentName:"li",href:"https://git-scm.com/book/en/v2/Getting-Started-Installing-Git"},"Git")),Object(n.b)("li",{parentName:"ul"},Object(n.b)("a",{parentName:"li",href:"https://docs.docker.com/install/"},"Docker")," (v19.03.5 or higher)"),Object(n.b)("li",{parentName:"ul"},Object(n.b)("a",{parentName:"li",href:"https://docs.docker.com/compose/install/#install-compose"},"Docker Compose for Linux")," (v1.25.0 or higher)")),Object(n.b)("h2",{id:"deploy-the-sandbox"},"Deploy the Sandbox"),Object(n.b)("ol",null,Object(n.b)("li",{parentName:"ol"},Object(n.b)("p",{parentName:"li"},"Log in to your host for the Sandbox.")),Object(n.b)("li",{parentName:"ol"},Object(n.b)("p",{parentName:"li"},"Clone the git repository for the HDFS Sandbox:"),Object(n.b)("pre",{parentName:"li"},Object(n.b)("code",{parentName:"pre",className:"language-text"},"git clone https://github.com/WANdisco/hdp-vanilla.git\n"))),Object(n.b)("li",{parentName:"ol"},Object(n.b)("p",{parentName:"li"},"Change directory to the repository:"),Object(n.b)("pre",{parentName:"li"},Object(n.b)("code",{parentName:"pre",className:"language-text"},"cd hdp-vanilla\n"))),Object(n.b)("li",{parentName:"ol"},Object(n.b)("p",{parentName:"li"},"Start the HDFS Sandbox"),Object(n.b)("pre",{parentName:"li"},Object(n.b)("code",{parentName:"pre",className:"language-text"},"docker-compose up -d\n")))),Object(n.b)("h2",{id:"install-and-configure-livedata-migrator"},"Install and configure LiveData Migrator"),Object(n.b)("ol",null,Object(n.b)("li",{parentName:"ol"},Object(n.b)("p",{parentName:"li"},"Download and install LiveData Migrator:"),Object(n.b)("pre",{parentName:"li"},Object(n.b)("code",{parentName:"pre",className:"language-text"},'docker-compose exec sandbox-hdp-vanilla bash -c "wget wandisco.com/downloads/livedata-migrator.sh -P /tmp; chmod +x /tmp/livedata-migrator.sh; /tmp/livedata-migrator.sh"\n'))),Object(n.b)("li",{parentName:"ol"},Object(n.b)("p",{parentName:"li"},"Open a browser window and access the LiveData UI using the Sandbox host IP on port 8081:"),Object(n.b)("pre",{parentName:"li"},Object(n.b)("code",{parentName:"pre",className:"language-text"},"http://sandbox_host_ip_address:8081\n"))),Object(n.b)("li",{parentName:"ol"},Object(n.b)("p",{parentName:"li"},"Fill out your account details and click ",Object(n.b)("strong",{parentName:"p"},"Register"),".")),Object(n.b)("li",{parentName:"ol"},Object(n.b)("p",{parentName:"li"},"Click the settings cog to configure your ",Object(n.b)("strong",{parentName:"p"},"Target")," storage.")),Object(n.b)("li",{parentName:"ol"},Object(n.b)("p",{parentName:"li"},"Select the ",Object(n.b)("strong",{parentName:"p"},"Storage Type")," as ",Object(n.b)("strong",{parentName:"p"},"S3")," and fill in the following details:"),Object(n.b)("ul",{parentName:"li"},Object(n.b)("li",{parentName:"ul"},Object(n.b)("strong",{parentName:"li"},"Storage Name")," - This is a user-provided identifier for the storage."),Object(n.b)("li",{parentName:"ul"},Object(n.b)("strong",{parentName:"li"},"Bucket Name")," - Your ",Object(n.b)("a",{parentName:"li",href:"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#create-bucket-intro"},"S3 bucket name"),"."),Object(n.b)("li",{parentName:"ul"},Object(n.b)("strong",{parentName:"li"},"Credentials Provider")," - Select the ",Object(n.b)("inlineCode",{parentName:"li"},"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")," option to provide an access and secret key."),Object(n.b)("li",{parentName:"ul"},Object(n.b)("strong",{parentName:"li"},"Access Key")," - The ",Object(n.b)("a",{parentName:"li",href:"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html"},"access key")," for the S3 bucket."),Object(n.b)("li",{parentName:"ul"},Object(n.b)("strong",{parentName:"li"},"Secret Key")," - The ",Object(n.b)("a",{parentName:"li",href:"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html"},"secret key")," for the S3 bucket.")),Object(n.b)("p",{parentName:"li"},"Click ",Object(n.b)("strong",{parentName:"p"},"Save")," once complete, and return to the dashboard."))),Object(n.b)("h2",{id:"start-a-migration"},"Start a migration"),Object(n.b)("ol",null,Object(n.b)("li",{parentName:"ol"},Object(n.b)("p",{parentName:"li"},"Click the ",Object(n.b)("strong",{parentName:"p"},"+")," in the ",Object(n.b)("strong",{parentName:"p"},"Rules/Migrations")," panel to create a new migration.")),Object(n.b)("li",{parentName:"ol"},Object(n.b)("p",{parentName:"li"},"Enter the migration details as follows:"),Object(n.b)("ul",{parentName:"li"},Object(n.b)("li",{parentName:"ul"},"Select the target storage as the one defined in the previous section (",Object(n.b)("strong",{parentName:"li"},"Storage Name"),")."),Object(n.b)("li",{parentName:"ul"},"Set the migration path as ",Object(n.b)("inlineCode",{parentName:"li"},"/retail_demo")),Object(n.b)("li",{parentName:"ul"},"Enable auto-start migration.")),Object(n.b)("p",{parentName:"li"},"Click ",Object(n.b)("strong",{parentName:"p"},"Create")," to begin the migration with the details provided."))),Object(n.b)("h2",{id:"confirm-the-migration-is-successful"},"Confirm the migration is successful"),Object(n.b)("p",null,"Once the migration is complete, check that the ",Object(n.b)("inlineCode",{parentName:"p"},"/retail_demo")," directory exists on your S3 bucket."),Object(n.b)("p",null,"A sub-directory should exist inside (",Object(n.b)("inlineCode",{parentName:"p"},"customer_addresses_dim_hive"),") with a ~50MB file inside (",Object(n.b)("inlineCode",{parentName:"p"},"customer_addresses_dim.tsv.gz"),")."))}s.isMDXComponent=!0},237:function(e,t,a){"use strict";a.d(t,"a",(function(){return p})),a.d(t,"b",(function(){return d}));var r=a(0),n=a.n(r);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function c(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var s=n.a.createContext({}),b=function(e){var t=n.a.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):c(c({},t),e)),a},p=function(e){var t=b(e.components);return n.a.createElement(s.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.a.createElement(n.a.Fragment,{},t)}},u=n.a.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,i=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),p=b(a),u=r,d=p["".concat(i,".").concat(u)]||p[u]||m[u]||o;return a?n.a.createElement(d,c(c({ref:t},s),{},{components:a})):n.a.createElement(d,c({ref:t},s))}));function d(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=u;var c={};for(var l in t)hasOwnProperty.call(t,l)&&(c[l]=t[l]);c.originalType=e,c.mdxType="string"==typeof e?e:r,i[1]=c;for(var s=2;s<o;s++)i[s]=a[s];return n.a.createElement.apply(null,i)}return n.a.createElement.apply(null,a)}u.displayName="MDXCreateElement"}}]);